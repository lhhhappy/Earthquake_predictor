{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Southern California', 'Japan', 'California']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/data2/linhang_data/Earthquake_data/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = \"Japan\"\n",
    "data_path = \"/data2/linhang_data/Earthquake_data/data/\"+area+\"/\"\n",
    "\n",
    "\n",
    "window_size = 14 * 102\n",
    "forecast_horizon = 14\n",
    "\n",
    "lape_dim = 30\n",
    "\n",
    "far_mask_delta = 30\n",
    "dtw_delta = 10\n",
    "\n",
    "\n",
    "gnss_data = pd.read_csv(data_path+\"gnss_data.csv\", index_col=0, parse_dates=True).map(parse_str_list)\n",
    "earthquake_data = pd.read_csv(data_path+\"earthquake_data.csv\", index_col=0, parse_dates=True)\n",
    "energy_data = pd.read_csv(data_path+\"energy_data.csv\", index_col=0, parse_dates=True)\n",
    "station_dict_use = pickle.load(open(data_path+\"station_dict_use.pkl\", \"rb\"))\n",
    "\n",
    "es_geo_matrix = pd.read_csv(data_path+\"es_geo_matrix.csv\", index_col=0)\n",
    "es_sem_matrix = pd.read_csv(data_path+\"es_sem_matrix.csv\", index_col=0)\n",
    "gnss_geo_matrix = pd.read_csv(data_path+\"gnss_geo_matrix.csv\", index_col=0)\n",
    "\n",
    "dataset_japan = EarthquakeGNSSDataset(area=area,earthquake_data=earthquake_data,es_geo_matrix=es_geo_matrix,es_sem_matrix=es_sem_matrix,\n",
    "                                gnss_geo_matrix=gnss_geo_matrix,gnss_data=gnss_data,far_mask_delta=far_mask_delta,\n",
    "                                dtw_delta=dtw_delta,lape_dim=lape_dim,\n",
    "                                window_size=window_size,forecast_horizon=forecast_horizon,earthquake_threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = \"California\"\n",
    "data_path = \"/data2/linhang_data/Earthquake_data/data/\"+area+\"/\"\n",
    "\n",
    "\n",
    "window_size = 14 * 102\n",
    "forecast_horizon = 14\n",
    "\n",
    "lape_dim = 30\n",
    "\n",
    "far_mask_delta = 30\n",
    "dtw_delta = 10\n",
    "\n",
    "\n",
    "gnss_data = pd.read_csv(data_path+\"gnss_data.csv\", index_col=0, parse_dates=True).map(parse_str_list)\n",
    "earthquake_data = pd.read_csv(data_path+\"earthquake_data.csv\", index_col=0, parse_dates=True)\n",
    "energy_data = pd.read_csv(data_path+\"energy_data.csv\", index_col=0, parse_dates=True)\n",
    "station_dict_use = pickle.load(open(data_path+\"station_dict_use.pkl\", \"rb\"))\n",
    "\n",
    "es_geo_matrix = pd.read_csv(data_path+\"es_geo_matrix.csv\", index_col=0)\n",
    "es_sem_matrix = pd.read_csv(data_path+\"es_sem_matrix.csv\", index_col=0)\n",
    "gnss_geo_matrix = pd.read_csv(data_path+\"gnss_geo_matrix.csv\", index_col=0)\n",
    "\n",
    "dataset_California = EarthquakeGNSSDataset(area=area,earthquake_data=earthquake_data,es_geo_matrix=es_geo_matrix,es_sem_matrix=es_sem_matrix,\n",
    "                                gnss_geo_matrix=gnss_geo_matrix,gnss_data=gnss_data,far_mask_delta=far_mask_delta,\n",
    "                                dtw_delta=dtw_delta,lape_dim=lape_dim,\n",
    "                                window_size=window_size,forecast_horizon=forecast_horizon,earthquake_threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_energy_history torch.Size([102, 100, 1])\n",
      "gnss_data_history torch.Size([1428, 334, 4])\n",
      "log_energy_future torch.Size([100, 1])\n",
      "earthquake_data_future_day torch.Size([100])\n",
      "es_geo_mask torch.Size([100, 100])\n",
      "es_sem_mask torch.Size([100, 100])\n",
      "gnss_geo_mask torch.Size([334, 334])\n",
      "lap_ex torch.Size([100, 30])\n",
      "lap_gnss torch.Size([334, 30])\n"
     ]
    }
   ],
   "source": [
    "for key in dataset_japan[0]:\n",
    "    print(key,dataset_japan[0][key].shape)\n",
    "mask = dataset_japan.get_masks()\n",
    "for key in mask:\n",
    "    print(key,mask[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_energy_history torch.Size([102, 100, 1])\n",
      "gnss_data_history torch.Size([1428, 294, 4])\n",
      "log_energy_future torch.Size([100, 1])\n",
      "earthquake_data_future_day torch.Size([100])\n",
      "es_geo_mask torch.Size([100, 100])\n",
      "es_sem_mask torch.Size([100, 100])\n",
      "gnss_geo_mask torch.Size([294, 294])\n",
      "lap_ex torch.Size([100, 30])\n",
      "lap_gnss torch.Size([294, 30])\n"
     ]
    }
   ],
   "source": [
    "for key in dataset_California[0]:\n",
    "    print(key,dataset_California[0][key].shape)\n",
    "mask = dataset_California.get_masks()\n",
    "for key in mask:\n",
    "    print(key,mask[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedEarthquakeGNSSDataset(Dataset):\n",
    "    def __init__(self, region_datasets):\n",
    "        \"\"\"\n",
    "        Class to combine datasets from different regions.\n",
    "        \n",
    "        :param region_datasets: Dictionary where keys are region names and values are instances of EarthquakeGNSSDataset.\n",
    "        \"\"\"\n",
    "        self.region_datasets = region_datasets\n",
    "        self.region_names = list(region_datasets.keys())\n",
    "        self.lengths = {region: len(ds) for region, ds in region_datasets.items()}\n",
    "        self.total_length = sum(self.lengths.values())\n",
    "        self.max_gnss_nodes = max(ds[0]['gnss_data_history'].shape[1] for ds in region_datasets.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve an item based on the global index. Maps the global index to a specific region dataset.\n",
    "        \"\"\"\n",
    "        for region in self.region_names:\n",
    "            if idx < self.lengths[region]:\n",
    "                data = self.region_datasets[region][idx]\n",
    "                data['region'] = region  # Include the region name in the data for identification.\n",
    "                return data\n",
    "            idx -= self.lengths[region]\n",
    "        raise IndexError(\"Index out of range in CombinedEarthquakeGNSSDataset.\")\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle batches with different numbers of GNSS nodes.\n",
    "        \n",
    "        :param batch: List of data samples from different regions.\n",
    "        :return: A dictionary containing combined batched data with region-specific masks.\n",
    "        \"\"\"\n",
    "        # Determine the maximum number of GNSS nodes in this batch.\n",
    "        max_gnss_nodes_in_batch = max(item['gnss_data_history'].shape[1] for item in batch)\n",
    "\n",
    "        # Initialize lists to store the padded data and masks.\n",
    "        log_energy_history = []\n",
    "        gnss_data_history = []\n",
    "        log_energy_future = []\n",
    "        earthquake_data_future_day = []\n",
    "        es_geo_masks = []\n",
    "        es_sem_masks = []\n",
    "        combined_gnss_masks = []\n",
    "        lap_ex_masks = []\n",
    "        lap_gnss_masks = []\n",
    "\n",
    "        # Pad each data sample's GNSS data and masks to match the maximum number of nodes.\n",
    "        for item in batch:\n",
    "            n_gnss_nodes = item['gnss_data_history'].shape[1]\n",
    "            pad_size = max_gnss_nodes_in_batch - n_gnss_nodes\n",
    "\n",
    "            # Pad the GNSS data along the node dimension (dim=1)\n",
    "            padded_gnss_data = F.pad(item['gnss_data_history'], (0, 0, 0, pad_size))\n",
    "            # Append the data.\n",
    "            log_energy_history.append(item['log_energy_history'])\n",
    "            gnss_data_history.append(padded_gnss_data)\n",
    "            log_energy_future.append(item['log_energy_future'])\n",
    "            earthquake_data_future_day.append(item['earthquake_data_future_day'])\n",
    "\n",
    "            # Retrieve region-specific masks.\n",
    "            region_name = item['region']\n",
    "            region_masks = self.region_datasets[region_name].get_masks()\n",
    "\n",
    "            # Pad 'lap_gnss' and 'gnss_geo_mask' to match the maximum number of nodes.\n",
    "            padded_gnss_geo_mask = F.pad(region_masks['gnss_geo_mask'], (0, pad_size,0, pad_size), value=1)\n",
    "\n",
    "            padded_lap_gnss = F.pad(region_masks['lap_gnss'], (0, 0, 0, pad_size))\n",
    "            # Combine GNSS padding mask with padded GNSS geo mask.\n",
    "            gnss_padding_mask = torch.cat([torch.ones(n_gnss_nodes), torch.zeros(pad_size)], dim=0)\n",
    "            combined_gnss_mask = gnss_padding_mask.unsqueeze(-1) * padded_gnss_geo_mask\n",
    "\n",
    "            # Store the masks.\n",
    "            es_geo_masks.append(region_masks['es_geo_mask'])\n",
    "            es_sem_masks.append(region_masks['es_sem_mask'])\n",
    "            combined_gnss_masks.append(combined_gnss_mask)\n",
    "            lap_ex_masks.append(region_masks['lap_ex'])\n",
    "            lap_gnss_masks.append(padded_lap_gnss)\n",
    "\n",
    "        # Stack the data to create tensors for the batch.\n",
    "        log_energy_history = torch.stack(log_energy_history)\n",
    "        gnss_data_history = torch.stack(gnss_data_history)\n",
    "        log_energy_future = torch.stack(log_energy_future)\n",
    "        earthquake_data_future_day = torch.stack(earthquake_data_future_day)\n",
    "\n",
    "        # Combine the padded masks.\n",
    "        es_geo_masks = torch.stack(es_geo_masks)\n",
    "        es_sem_masks = torch.stack(es_sem_masks)\n",
    "        combined_gnss_masks = torch.stack(combined_gnss_masks)\n",
    "        lap_ex_masks = torch.stack(lap_ex_masks)\n",
    "        lap_gnss_masks = torch.stack(lap_gnss_masks)\n",
    "\n",
    "        # Combine into a dictionary.\n",
    "        batch_data = {\n",
    "            'log_energy_history': log_energy_history,\n",
    "            'gnss_data_history': gnss_data_history,\n",
    "            'log_energy_future': log_energy_future,\n",
    "            'earthquake_data_future_day': earthquake_data_future_day,\n",
    "            'es_geo_mask': es_geo_masks,\n",
    "            'es_sem_mask': es_sem_masks,\n",
    "            'combined_gnss_mask': combined_gnss_masks,\n",
    "            'lap_ex': lap_ex_masks,\n",
    "            'lap_gnss': lap_gnss_masks\n",
    "        }\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "# Example usage:\n",
    "# region_datasets = {\n",
    "#     'region_1': dataset_japan,\n",
    "#     'region_2': dataset_california,\n",
    "# }\n",
    "# combined_dataset = CombinedEarthquakeGNSSDataset(region_datasets)\n",
    "# dataloader = DataLoader(combined_dataset, batch_size=32, collate_fn=combined_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_datasets = {\"Japan\": dataset_japan, \"California\": dataset_California}\n",
    "dataset = CombinedEarthquakeGNSSDataset(region_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 40 20 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset length:\", len(dataset),region_datasets[\"Japan\"].__len__(),region_datasets[\"California\"].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([334]) torch.Size([334, 334])\n",
      "torch.Size([334]) torch.Size([334, 334])\n",
      "torch.Size([334]) torch.Size([334, 334])\n",
      "torch.Size([334]) torch.Size([334, 334])\n",
      "torch.Size([334]) torch.Size([334, 334])\n",
      "torch.Size([334]) torch.Size([334, 334])\n",
      "torch.Size([334]) torch.Size([334, 334])\n",
      "torch.Size([334]) torch.Size([334, 334])\n",
      "Batch data:\n",
      "log_energy_history torch.Size([8, 102, 100, 1])\n",
      "gnss_data_history torch.Size([8, 1428, 334, 4])\n",
      "log_energy_future torch.Size([8, 100, 1])\n",
      "earthquake_data_future_day torch.Size([8, 100])\n",
      "es_geo_mask torch.Size([8, 100, 100])\n",
      "es_sem_mask torch.Size([8, 100, 100])\n",
      "combined_gnss_mask torch.Size([8, 334, 334])\n",
      "lap_ex torch.Size([8, 100, 30])\n",
      "lap_gnss torch.Size([8, 334, 30])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "for batch in dataloader:\n",
    "    print(\"Batch data:\")\n",
    "    for key, value in batch.items():\n",
    "        print(key, value.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
